@article{AgarwalLearnQueries,
author = {Agarwal, Saurabh and Styles, Luke and Verma, Saurabh},
title = {{Learn to Rank ICDM 2013 Challenge Ranking Hotel Search Queries}},
url = {http://www-users.cs.umn.edu/{~}verma/letor.pdf}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Freund and Schapire[1996]), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
file = {:home/elena/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 2001 - Random Forests.pdf:pdf},
title = {{Random Forests}},
url = {https://www.stat.berkeley.edu/{~}breiman/randomforest2001.pdf},
year = {2001}
}
@article{MART,
abstract = {LambdaMART is the boosted tree version of LambdaRank, which is based on RankNet. RankNet, LambdaRank, and LambdaMART have proven to be very suc-cessful algorithms for solving real world ranking problems: for example an ensem-ble of LambdaMART rankers won Track 1 of the 2010 Yahoo! Learning To Rank Challenge. The details of these algorithms are spread across several papers and re-ports, and so here we give a self-contained, detailed and complete description of them.},
author = {Burges, Christopher J C},
file = {:home/elena/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burges - Unknown - From RankNet to LambdaRank to LambdaMART An Overview.pdf:pdf},
title = {{From RankNet to LambdaRank to LambdaMART: An Overview}},
url = {https://pdfs.semanticscholar.org/0df9/c70875783a73ce1e933079f328e8cf5e9ea2.pdf}
}
@article{Cao2007LearningApproach,
author = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
title = {{Learning to Rank: From Pairwise Approach to Listwise Approach}},
url = {http://www.machinelearning.org/proceedings/icml2007/papers/139.pdf},
year = {2007}
}
@book{Cristianini2000,
abstract = {The learning methodology -- Linear learning machines -- Kernal-induced feature spaces -- Generalisation theory -- Optimisation theory -- Support vector machines -- Implementation techniques -- Application of support vector machines -- Pseudocode for the SMO algorithm -- Background mathematics.},
author = {Cristianini, Nello. and Shawe-Taylor, John.},
isbn = {0521780195},
pages = {189},
publisher = {Cambridge University Press},
title = {{An introduction to support vector machines : and other kernel-based learning methods}},
url = {https://www.aaai.org/ojs/index.php/aimagazine/article/view/1710},
year = {2000}
}
@article{Liu2007LearningRetrieval,
author = {Liu, Tie-Yan},
doi = {10.1561/1500000016},
issn = {1554-0669},
journal = {Foundations and Trends{\{}{\textregistered}{\}} in Information Retrieval},
keywords = {Information Retrieval,Machine Learning,Metasearch,Statistical learning theory,rank aggregation and data fusion},
number = {3},
pages = {225--331},
publisher = {Now Publishers, Inc.},
title = {{Learning to Rank for Information Retrieval}},
url = {http://www.nowpublishers.com/article/Details/INR-016},
volume = {3},
year = {2007}
}
@article{McKinney2015PandasToolkit,
author = {McKinney, Wes and Team, PyData Development},
journal = {Pandas - Powerful Python Data Analysis Toolkit},
keywords = {Analysis,Data,Pandas,Powerful,Python,Toolkit},
title = {{Pandas - Powerful Python Data Analysis Toolkit}},
year = {2015}
}
@misc{Parachuri,
author = {Parachuri, Vik},
title = {{How to get into the top 15 of a Kaggle competition using Python}},
url = {https://www.dataquest.io/blog/kaggle-tutorial/},
urldate = {2017-05-28}
}
@article{Pedregosa2011Scikit-learn:Python,
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
issn = {ISSN 1533-7928},
journal = {Journal of Machine Learning Research},
number = {Oct},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html},
volume = {12},
year = {2011}
}
@article{CA,
abstract = {Coordinate descent algorithms solve optimization problems by suc-cessively performing approximate minimization along coordinate directions or coordinate hyperplanes. They have been used in applications for many years, and their popularity continues to grow because of their usefulness in data anal-ysis, machine learning, and other areas of current interest. This paper describes the fundamentals of the coordinate descent approach, together with variants and extensions and their convergence properties, mostly with reference to con-vex objectives. We pay particular attention to a certain problem structure that arises frequently in machine learning applications, showing that efficient im-plementations of accelerated coordinate descent algorithms are possible for problems of this type. We also present some parallel variants and discuss their convergence properties under several models of parallel execution.},
author = {Wright, Stephen J},
file = {:home/elena/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wright - Unknown - Coordinate Descent Algorithms.pdf:pdf},
title = {{Coordinate Descent Algorithms}},
url = {https://arxiv.org/pdf/1502.04759.pdf}
}
@article{Ada,
abstract = {In this paper we address the issue of learning to rank for document retrieval. In the task, a model is automatically created with some training data and then is utilized for ranking of documents. The goodness of a model is usually evaluated with performance mea-sures such as MAP (Mean Average Precision) and NDCG (Nor-malized Discounted Cumulative Gain). Ideally a learning algo-rithm would train a ranking model that could directly optimize the performance measures with respect to the training data. Existing methods, however, are only able to train ranking models by mini-mizing loss functions loosely related to the performance measures. For example, Ranking SVM and RankBoost train ranking mod-els by minimizing classification errors on instance pairs. To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures. Our algorithm, re-ferred to as AdaRank, repeatedly constructs 'weak rankers' on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions. We prove that the training process of AdaRank is exactly that of enhancing the per-formance measure used. Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.},
author = {Xu, Jun and Li, Hang},
file = {:home/elena/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Li - Unknown - AdaRank A Boosting Algorithm for Information Retrieval.pdf:pdf},
keywords = {Boosting,Experimentation,Learning to rank,Retrieval models General Terms Algorithms,Theory Keywords Information retrieval},
title = {{AdaRank: A Boosting Algorithm for Information Retrieval}},
url = {http://www.bigdatalab.ac.cn/{~}junxu/publications/SIGIR2007{\_}AdaRank.pdf}
}
@misc{ExpediaHttps://www.kaggle.com/c/expedia-hotel-recommendations,
title = {{Expedia 2016 Kaggle competition. https://www.kaggle.com/c/expedia-hotel-recommendations}},
url = {https://www.kaggle.com/c/expedia-hotel-recommendations}
}
@misc{2017ExpediaResults,
title = {{Expedia, Inc. Reports Fourth Quarter and Full Year 2016 Results}},
url = {http://files.shareholder.com/downloads/EXPE/3880984722x0x927398/D6FADF0D-8265-4833-9C34-8BF23AFA21E6/Q4{\_}2016{\_}Earnings{\_}Release.pdf},
year = {2017}
}
@misc{ExpediaHttps://www.kaggle.com/c/expedia-personalized-sort,
title = {{Expedia 2013 Kaggle competition. https://www.kaggle.com/c/expedia-personalized-sort}},
url = {https://www.kaggle.com/c/expedia-personalized-sort}
}
@misc{ExpediaHttps://www.kaggle.com/c/expedia-personalized-sort/discussion/6203,
title = {{Expedia 2013 Kaggle competition winners slides - https://www.kaggle.com/c/expedia-personalized-sort/discussion/6203}},
url = {https://www.kaggle.com/c/expedia-personalized-sort/discussion/6203}
}
@misc{Kaggle21646,
title = {{Expedia Hotel Recommendations | Kaggle}},
url = {https://www.kaggle.com/c/expedia-hotel-recommendations/discussion/21646},
urldate = {2017-05-28}
}
@misc{Kaggle21588,
title = {{Expedia Hotel Recommendations | Kaggle}},
url = {https://www.kaggle.com/c/expedia-hotel-recommendations/discussion/21588},
urldate = {2017-05-28}
}
@misc{Kaggle21607,
title = {{Expedia Hotel Recommendations | Kaggle}},
url = {https://www.kaggle.com/c/expedia-hotel-recommendations/discussion/21607},
urldate = {2017-05-28}
}
@misc{XGB,
title = {{Introduction to Boosted Trees â€” xgboost 0.6 documentation}},
url = {http://xgboost.readthedocs.io/en/latest/model.html},
urldate = {2017-05-28}
}
@misc{NDCG,
title = {{Normalized Discounted Cumulative Gain | Kaggle}},
url = {https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain},
urldate = {2017-05-28}
}
@misc{Lemur,
title = {{The Lemur Project / Wiki / RankLib}},
url = {https://sourceforge.net/p/lemur/wiki/RankLib/},
urldate = {2017-05-28}
}
